{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# a_tensor_initialization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T11:12:58.337715Z",
     "start_time": "2023-09-05T11:12:57.617134Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T11:13:06.986233Z",
     "start_time": "2023-09-05T11:13:06.911492Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)\n",
    "print(t1.device)\n",
    "print(t1.requires_grad)\n",
    "print(t1.size())\n",
    "print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T11:13:33.775422Z",
     "start_time": "2023-09-05T11:13:33.751252Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)\n",
    "print(t2.device)\n",
    "print(t2.requires_grad)\n",
    "print(t2.size())\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    ".Tensor는 데이터타입을 float형으로 바꿈\n",
    "\n",
    ".tensor는 데이터타입을 그대로 유지시켜줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([]) \n",
      "ndims :  0\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(1)\n",
    "print('shape : ', a1.shape, '\\nndims : ', a1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([1]) \n",
      "ndims :  1\n"
     ]
    }
   ],
   "source": [
    "a2 = torch.tensor([1])\n",
    "print('shape : ', a2.shape, '\\nndims : ', a2.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([5]) \n",
      "ndims :  1\n"
     ]
    }
   ],
   "source": [
    "a3 = torch.tensor([1, 2, 3, 4, 5])\n",
    "print('shape : ', a3.shape, '\\nndims : ', a3.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([5, 1]) \n",
      "ndims :  2\n"
     ]
    }
   ],
   "source": [
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])\n",
    "print('shape : ', a4.shape, '\\nndims : ', a4.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([3, 2]) \n",
      "ndims :  2\n"
     ]
    }
   ],
   "source": [
    "a5 = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print('shape : ', a5.shape, '\\nndims : ', a5.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([3, 2, 1]) \n",
      "ndims :  3\n"
     ]
    }
   ],
   "source": [
    "a6 = torch.tensor([\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print('shape : ', a6.shape, '\\nndims : ', a6.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([3, 1, 2, 1]) \n",
      "ndims :  4\n"
     ]
    }
   ],
   "source": [
    "a7 = torch.tensor([\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print('shape : ', a7.shape, '\\nndims : ', a7.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([3, 1, 2, 3]) \n",
      "ndims :  4\n"
     ]
    }
   ],
   "source": [
    "a8 = torch.tensor([\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print('shape : ', a8.shape, '\\nndims : ', a8.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([3, 1, 2, 3, 1]) \n",
      "ndims :  5\n"
     ]
    }
   ],
   "source": [
    "a9 = torch.tensor([\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print('shape : ', a9.shape, '\\nndims : ', a9.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([4, 5]) \n",
      "ndims :  2\n"
     ]
    }
   ],
   "source": [
    "a10 = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    ])\n",
    "print('shape : ', a10.shape, '\\nndims : ', a10.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([4, 1, 5]) \n",
      "ndims :  3\n"
     ]
    }
   ],
   "source": [
    "a11 = torch.tensor([\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    ])\n",
    "print('shape : ', a11.shape, '\\nndims : ', a11.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# a12 = torch.tensor([\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "#     ])\n",
    "# print('shape : ', a11.shape, '\\nndims : ', a11.ndim)  # runall이 오류에 막혀서 실행시킨 후 주석처리하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "a12 텐서에서는 같은 디멘션에서의 길이(원소의 개수)가 달라 에러가 발생한 모습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# b_tensor_initialization_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "t1, t2, t3는 각각 l1, l2, l3를 copy하여 새로 생성한 객체이기 때문에 l1, l2, l3를 바꿔도 t1, t2, t3는 영향을 받지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "source": [
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "t4, t5도 l4, l5를 copy하여 생성한 객체이기 때문에 l4, l5를 바꿔도 t4, t5가 영향을 받지 않지만, numpy 배열을 as_tensor로 가져올 땐 l6객체를 공유하기 때문에\n",
    "l6의 값을 바꾸면 t6의 값도 같이 바뀌게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# c_tensor_initialization_constant_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 :  tensor([1., 1., 1., 1., 1.])\n",
      "t1_like :  tensor([1., 1., 1., 1., 1.])\n",
      "l1 :  tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "l1_like :  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(size=(5,))  # (5,) 크기의 텐서를 1로 가득 채움\n",
    "t1_like = torch.ones_like(input=t1) # input 데이터의 크기와 모양만 가져와 1로 가득 채움\n",
    "print('t1 : ', t1)\n",
    "print('t1_like : ', t1_like)\n",
    "\n",
    "l1 = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "l1_like = torch.ones_like(l1)\n",
    "print('l1 : ', l1)\n",
    "print('l1_like : ', l1_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "l1_like가 입력으로 받은 l1의 값 자체는 무시하고 크기와 모양만 가져와 내부 값을 모두 1로 만든 모습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2 :  tensor([0., 0., 0., 0., 0., 0.])\n",
      "t2_like :  tensor([0., 0., 0., 0., 0., 0.])\n",
      "l2 :  tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "l2_like :  tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.zeros(size=(6,)) # (6,) 크기의 텐서를 0으로 가득 채움\n",
    "t2_like = torch.zeros_like(input=t2) # input 데이터의 크기와 모양만 가져와 0으로 가득 채움\n",
    "print('t2 : ', t2)\n",
    "print('t2_like : ', t2_like)\n",
    "\n",
    "l2 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "l2_like = torch.zeros_like(l2)\n",
    "print('l2 : ', l2)\n",
    "print('l2_like : ', l2_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3 :  tensor([0., 0., 0., 0.])\n",
      "t3_like :  tensor([0., 0., 0., 0.])\n",
      "l3 :  tensor([1, 3, 2, 1, 5, 6, 1, 2])\n",
      "l3_like :  tensor([0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.empty(size=(4,)) # (4,) 크기의 텐서를 초기화되지 않은 값으로 가득 채움\n",
    "t3_like = torch.empty_like(input=t3) # input 데이터의 크기와 모양을 가져와 빈값(0)으로 가득 채움. 해당 값들은 초기화되지 않은 값이기 때문에 사용전에 초기화를 해줘야함\n",
    "print('t3 : ', t3)\n",
    "print('t3_like : ', t3_like)\n",
    "\n",
    "l3 = torch.tensor([1, 3, 2, 1, 5, 6, 1, 2])\n",
    "l3_like = torch.empty_like(l3)\n",
    "print('l3 : ', l3)\n",
    "print('l3_like : ', l3_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.eye(n=3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "torch.eye는 입력받는 n의 값에 따라 n*n 크기의 단위행렬을 생성해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# d_tensor_initialization_random_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "tensor([[13, 16]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randint(low=10, high=20, size=(1, 2)) # (1, 2) 사이즈의 텐서를 최소 10 최대 20 사이의 랜덤 int 값으로 채움\n",
    "print(t1.dtype)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[0.7060, 0.8836, 0.7397]])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.rand(size=(1, 3)) # (1, 3) 사이즈의 텐서를 0에서 1사이의 랜덤한 실수 float 값으로 채움\n",
    "print(t2.dtype)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1.3155, 1.1747, 2.8174]])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.randn(size=(1, 3)) # (1, 3)사이즈의 텐서를 표준 정규 분포에서 평균이 0이고 표준편차가 1인 랜덤한 실수 float 값으로 채움\n",
    "print(t3.dtype)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[ 9.7326, 11.0743],\n",
      "        [ 8.9923, 11.6500],\n",
      "        [ 9.3798,  9.8218]])\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2)) # (3, 2)사이즈의 텐서를 가우시안 정규분포 내에서 평균 10, 표준편차 1인 랜덤한 실수 float값으로 채움\n",
    "print(t4.dtype)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([0.0000, 2.5000, 5.0000])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3) # 0부터 5를 시작, 중간, 끝으로 3등분한 실수 float 값으로 텐서 생성\n",
    "print(t5.dtype)\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "t6 = torch.arange(5) # 0에서 4까지의 인트값으로 텐서 생성\n",
    "print(t6.dtype)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729) # 랜덤값이 이 시드에 의해 결정되어 시드를 정해주면 랜덤값이 고정됨\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "random2 = torch.rand(2, 3)\n",
    "print(random2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729) # 랜덤값이 이 시드에 의해 결정되어 시드를 정해주면 랜덤값이 고정됨\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# e_tensor_type_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type :  torch.float32\n",
      "a :  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3))\n",
    "print('type : ', a.dtype) # ones의 data type은 float형이 default\n",
    "print('a : ', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "b = torch.ones((2, 3), dtype=torch.int16) # dtype을 인수로 넣어 형태를 정해줄 수 있음\n",
    "print(b) # 결과에 dtype도 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "d = b.to(torch.int32) # b 그대로 가져와서 int32형으로 객체 d 생성\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "double_d :  tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n",
      "short_e :  tensor([[1, 2]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "print('double_d : ', double_d)\n",
    "print('short_e : ', short_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "double_d :  tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], dtype=torch.float64)\n",
      "short_e :  tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "print('double_d : ', double_d)\n",
    "print('short_e : ', short_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "double_d :  tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], dtype=torch.float64)\n",
      "short_e :  tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "print('double_d : ', double_d)\n",
    "print('short_e : ', short_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "double_d :  tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], dtype=torch.float64)\n",
      "short_e :  tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2).type(dtype=torch.short)\n",
    "print('double_d : ', double_d)\n",
    "print('short_e : ', short_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "dtype을 인수로 지정하거나 여러 메서드를 이용하여 전체 데이터에 타입을 브로드캐스트시킬 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "double_f = torch.rand(5, dtype=torch.double) # 5개의 double형 랜덤값(0~1)으로 이뤄진 텐서 생성\n",
    "short_g = double_f.to(torch.short) # double_f가져와서 short형으로 텐서 생성\n",
    "print((double_f*short_g).dtype) # 결과 : double * short 하니 결과값이 double형으로 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# f_tensor_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3 :  tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "t4 :  tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2)  # .add(a, b)는 a와 b텐서의 합을 구하는데 각 두 텐서의 사이즈가 다르면 오류 발생\n",
    "t4 = t1 + t2\n",
    "print('t3 : ', t3)\n",
    "print('t4 : ', t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5 :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "t6 :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.sub(t1, t2) # .sub(a, b)는 a와 b텐서의 차를 구하는데 두 텐서의 사이즈가 다르면 오류 발생\n",
    "t6 = t1 - t2\n",
    "print('t5 : ', t5)\n",
    "print('t6 : ', t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t7 :  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "t8 :  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.mul(t1, t2) # .mul(a, b)는 a와 b의 곱을 구하는데 두 텐서의 사이즈가 다르면 오류 발생\n",
    "t8 = t1 * t2\n",
    "print('t7 : ', t7)\n",
    "print('t8 : ', t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t9 :  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "t10 :  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.div(t1, t2) # .div(a, b)는 a를 b로 나눈 몫을 구하는데 두 텐서의 사이즈가 다르면 오류 발생\n",
    "t10 = t1 / t2\n",
    "print('t9 : ', t9)\n",
    "print('t10 : ', t10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# g_tensor_operations_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.dot(\n",
    "    torch.tensor([2, 3]), torch.tensor([2, 1]) # [a1, b1], [a2, b2]의 dot product연산 결과는 a1*b1 + a2*b2\n",
    ")\n",
    "print(t1, t1.size()) # 연산 결과로 텐서들을 하나의 값으로 만들었기 때문에 결과값은 스칼라값이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1766, -0.3342],\n",
      "        [-3.0611, -0.5943]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3) # (a, b), (c, d)사이즈의 matmul 연산 결과는 (a, d)사이즈가 됨\n",
    "print(t4, t4.size()) # 곱셈 시 브로드캐스트를 지원하지 않으므로 b와 c의 값을 맞춰줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6) # 인수 가장 앞의 배치값을 제외하면 mm메서드와 사용법이 동일\n",
    "print(t7.size()) # 연산 : (a, b, c), (a, c, d) 결과 : (a, b, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# h_tensor_operations_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn(3) # vector\n",
    "t2 = torch.randn(3) # vector\n",
    "print(torch.matmul(t1, t2).size()) # Vector와 Vector의 곱은 두 벡터의 dot product 결과와 같음 = 스칼라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.randn(3, 4) # matrix\n",
    "t4 = torch.randn(4) # vector\n",
    "print(torch.matmul(t3, t4).size()) # matrix(a, b)와 vector(b)의 곱은 broadcast dot을 통해 (a)사이즈의 텐서만 남음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.randn(10, 3, 4) # batched matrix\n",
    "t6 = torch.randn(4) # vector\n",
    "print(torch.matmul(t5, t6).size()) # batched matrix는 matrix에 batch가 붙은걸 제외하면 matrix와 vector간의 연산과 크게 다르지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.randn(10, 3, 4) # batched matrix\n",
    "t8 = torch.randn(10, 4, 5) # batched matrix\n",
    "print(torch.matmul(t7, t8).size()) # batched matrix간의 matmul연산도 위에서 했던것 처럼 (a, b, c), (a, c, d)이면 (a, b, d)로 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.randn(10, 3, 4) # batched matrix\n",
    "t10 = torch.randn(4, 5) # matrix\n",
    "print(torch.matmul(t9, t10).size()) # batched와 그냥 matrix를 matmul해주면 batch는 그대로 두고 뒤의 matrix끼리 dot연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# i_tensor_broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2) # broadcasting해줘서 텐서 전체에 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_size :  torch.Size([3, 2])\n",
      "t3 - t4 size :  torch.Size([3, 2])\n",
      "t3 - t4 : tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print('t3_size : ', t3.size())\n",
    "print('t3 - t4 size : ', (t3 - t4).size())\n",
    "print('t3 - t4 :', t3 - t4)\n",
    "# 차도 broadcasting해서 t3의 size가 유지됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)\n",
    "print(t5 - 2.0)\n",
    "print(t5 * 2.0)\n",
    "print(t5 / 2.0)\n",
    "# 사칙연산 모두 broadcast된 모습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size()) # normalization은 사용자가 데이터를 원하는 범위로 제한하기 위해 사용함 ex) 0~1사이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor([[1, 2], [0, 3]]) # size : (2, 2)\n",
    "t8 = torch.tensor([[3, 1]]) # size : (1, 2)\n",
    "t9 = torch.tensor([[5], [2]]) # size : (2, 1)\n",
    "t10 = torch.tensor([7]) # size : (1)\n",
    "\n",
    "print(t7 + t8)\n",
    "print(t7 + t9)\n",
    "print(t8 + t9)\n",
    "print(t7 + t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)# 텐서끼리 곱할 때 사이즈를 뒤로 붙여서 계산한다고 봐야함 ex)   (4, 3, 2)\n",
    "print(t12.shape) #                                                           (   3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "t13 = torch.ones(4, 3, 2)   # 위와 같이 뒤로 붙여서 계산하며, 텐서의 곱에서는 같은 dim의 위치끼리 size가 같아야함\n",
    "t14 = t13 * torch.rand(3, 1)\n",
    "print(t14.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)\n",
    "print(t16.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)\n",
    "print((t17 + t18).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# t25 = torch.empty(5, 2, 4, 1) # 차원의 수가 더 많은 텐서의 size가 적은 텐서보다 커야함\n",
    "# t26 = torch.empty(3, 1, 1) # (2, 4, 1) vs (3, 1, 1)에서 앞의 텐서가 2로 더 작아서 에러 발생\n",
    "# print((t25 + t26).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "t27 = torch.ones(4) * 5\n",
    "print(t27) # 5를 tensor(4)에 broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25., 25., 25., 25.])\n"
     ]
    }
   ],
   "source": [
    "t28 = torch.pow(t27, 2)\n",
    "print(t28) # 제곱연산도 broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "exp = torch.arange(1., 5.) # torch.tensor([1., 2., 3., 4.])\n",
    "a = torch.arange(1., 5.) # torch.tensor([1., 2., 3., 4.])\n",
    "t29 = torch.pow(a, exp) # 순서에 맞게 exp가 밑, a가 지수\n",
    "print(t29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# j_tensor_indexing_slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "    [[0, 1, 2, 3, 4],\n",
    "     [5, 6, 7, 8, 9],\n",
    "     [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1]) # (3, 5)에서 인덱스를 하나만 지정해줬으므로 앞의 인덱스 1일 때 출력\n",
    "print(x[:, 1]) # 세로 인덱스 1일 때 출력\n",
    "print(x[1, 2]) # (3, 5)에 맞춰 각각 인덱스 1, 2일 때 출력\n",
    "print(x[:, -1]) # 세로인덱스인데 -1이므로 뒤에서 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:]) # (3, 5)에서 앞의 인덱스 1부터 출력\n",
    "print(x[1:, 3:]) # 앞의 인덱스는 1부터, 뒤의 인덱스는 3부터 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1 # 1:4이므로 (6, 6)에서 앞의 인덱스 1, 2, 3인 경우이고, 세로 인덱스 2인 값들 1로 만듬\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(y[1:4, 1:4]) # 가로 세로 인덱스 1, 2, 3만 뽑아서 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor(\n",
    "    [[1, 2, 3, 4],\n",
    "     [2, 3, 4, 5],\n",
    "     [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# k_tensor_reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)\n",
    "t3 = t1.reshape(1, 6) # view나 reshape은 모양 바꾸고 copy만들어서 생성\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.arange(8).view(2, 4) # 0~7까지 텐서 만들고 (2, 4)사이즈로 생성\n",
    "t5 = torch.arange(6).view(2, 3) # 0~5까지 텐서 만들고 (2, 3)사이즈로 생성\n",
    "print(t4)\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "t6 = torch.tensor([[[1], [2], [3]]]) # (1, 3, 1)\n",
    "t7 = t6.squeeze() # size = 1 인 dimension 모두 삭제 >>(3)\n",
    "t8 = t6.squeeze(0) # 인덱스가 0인 dimension 삭제 >> (3, 1)\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.tensor([1, 2, 3]) # (3, )\n",
    "t10 = t9.unsqueeze(1) # 인덱스 1인 dimension 추가 >> (3, 1)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "t11 = torch.tensor( # (2, 3)\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # 인덱스 1인 dimension 추가 >> (2, 1, 3)\n",
    "print(t12, t12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]]) # (2, 3)\n",
    "t14 = t13.flatten() # []로 묶여있던 텐서들을 한줄(?)로 풀어줌 >> (6, )\n",
    "print(t14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]]) # (2, 2, 2)\n",
    "t16 = torch.flatten(t15) # >>(6, )\n",
    "t17 = torch.flatten(t15, start_dim=1) # 인덱스 1인 dimension부터 flatten >> (2, 4)\n",
    "print(t16)\n",
    "print(t17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  torch.Size([2, 3, 5])\n",
      "permute :  torch.Size([5, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t18 = torch.randn(2, 3, 5)\n",
    "print('shape : ', t18.shape)  # (2, 3, 5)\n",
    "print('permute : ', torch.permute(t18, (2, 0, 1)).shape) # 순서 바꾸기로, 2번째 인수인 (2, 0, 1)은 각각 원래 dimension의 순서를 (2번째, 0번째, 1번째)로 바꾸라는 얘기임 >> (5, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]]) # (2, 3)\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # 0번째, 1번째이니 (2, 3)그대로 유지\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # 1번째, 0번째이니 (3, 2)로 변경\n",
    "print(t20)\n",
    "print(t21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t22 = torch.transpose(t19, 0, 1) # t19의 0번째와 1번째 인덱스 교환이니 (3, 2)로 변경\n",
    "print(t22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t23 = torch.t(t19) # 2d텐서 0, 1 인덱스 반전시켜 (3, 2)\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# l_tensor_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1) # dim 인수에 들어가있는 dimension을 기준으로 텐서 통합\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.arange(0, 3)  # [0, 1, 2]\n",
    "t6 = torch.arange(3, 8)  # [3, 4, 5, 6, 7]\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0) # >> [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "print(t7.shape) # (8)\n",
    "print(t7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "t8 = torch.arange(0, 6).reshape(2, 3)  # [[0, 1, 2], [3, 4, 5]]\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # [[6, 7, 8],[9, 10, 11]]\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0) # dim = 0 기준으로 통합 >> [[0~2], [3~5], [6~8], [9~11]]\n",
    "print(t10.size())  # (4, 3)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1) # dim = 1을 기준으로 통합 >> [[0~2, 6~8], [3~5, 9~11]]\n",
    "print(t11.size())  # (2, 6)\n",
    "print(t11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "t12 = torch.arange(0, 6).reshape(2, 3)  # [[0, 1, 2], [3, 4, 5]]\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # [[6, 7, 8],[9, 10, 11]]\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # [[12, 13, 14],[15, 16, 17]]\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0) # dim = 0 기준 통합\n",
    "print(t15.size())  # (6, 3)\n",
    "print(t15)\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1) # dim = 1 기준으로 통합\n",
    "print(t16.size())  # (2, 9)\n",
    "print(t16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # [[[0, 1, 2], [3, 4, 5]]]\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # [[[6, 7, 8], [9, 10, 11]]]\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0) # dim = 0 기준 통합\n",
    "print(t19.size())  # (2, 2, 3)\n",
    "print(t19)\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1) # dim = 1 기준 통합\n",
    "print(t20.size())  # (1, 4, 3)\n",
    "print(t20)\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2) # dim = 2 기준 통합\n",
    "print(t21.size())  # (1, 2, 6)\n",
    "print(t21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# m_tensor_stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]]) # (2, 3)\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]]) # (2, 3)\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0) # dim = 0 차원을 새로 추가하고 기준으로 삼아 통합\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0) # 각 텐서에 dim = 0 기준으로 unsqeeze시키고 통합하면 stack과 같은 결과가 나옴\n",
    "print(t3.shape, t3.equal(t4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.stack([t1, t2], dim=1) # dim = 1 기준 통합\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1) # 위와 같은데 dim만 1을 기준으로\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2) # dim = 2 기준 통합\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2) # 위와 같은데 dim만 2를 기준으로\n",
    "print(t7.shape, t7.equal(t8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.arange(0, 3)  # [0, 1, 2] > (3)\n",
    "t10 = torch.arange(3, 6)  # [3, 4, 5] > (3)\n",
    "print(t9.size(), t10.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "t11 = torch.stack((t9, t10), dim=0) # dim = 0 기준 통합\n",
    "print(t11.size())  # (2, 3)\n",
    "print(t11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "t13 = torch.stack((t9, t10), dim=1) # dim = 1 기준 통합\n",
    "print(t13.size())  # (3,2)\n",
    "print(t13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# n_tensor_vstack_hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3]) # (3)\n",
    "t2 = torch.tensor([4, 5, 6]) # (3)\n",
    "t3 = torch.vstack((t1, t2)) # vertically stack > row wise > (column 수가 같아야 함)\n",
    "print(t3)\n",
    "print(t3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6]])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.tensor([[1], [2], [3]]) # (3, 1)\n",
    "t5 = torch.tensor([[4], [5], [6]]) # (3, 1)\n",
    "t6 = torch.vstack((t4, t5))\n",
    "print(t6)\n",
    "print(t6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor([  # (2, 2, 3)\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "\n",
    "t8 = torch.tensor([ # (2, 2, 3)\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "\n",
    "t9 = torch.vstack([t7, t8]) # vertically stack >> (4, 2, 3)\n",
    "print(t9.shape)\n",
    "print(t9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "t10 = torch.tensor([1, 2, 3]) # (3)\n",
    "t11 = torch.tensor([4, 5, 6]) # (3)\n",
    "t12 = torch.hstack((t10, t11)) # horizontally stack > column wise > (row 수가 같아야 함)\n",
    "print(t12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t13 = torch.tensor([[1], [2], [3]]) # (3, 1)\n",
    "t14 = torch.tensor([[4], [5], [6]]) # (3, 1)\n",
    "t15 = torch.hstack((t13, t14)) # horizontally stack >> (3, 2)\n",
    "print(t15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t16 = torch.tensor([ # (2, 2, 3)\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "\n",
    "t17 = torch.tensor([ # (2, 2, 3)\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "\n",
    "t18 = torch.hstack([t16, t17]) # horizontally stack >> (2, 4, 3)\n",
    "print(t18.shape)\n",
    "print(t18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 숙제 후기\n",
    "\n",
    "## 수업으로만 들었을 때와 직접 코드를 쳐봤을 때 이해의 깊이가 다르다는 것을 깨달았습니다. 눈으로만 봤을 때는 코드의 결과가 이해가 잘 안가는 경우가 조금 있었는데, 손으로 직접 코드를 쳐보니 코드를 완성해가는 과정에서 계속해서 지금 치고있는 코드의 결과가 어떻게 될지에 대해서 생각하게 되고, 다음에 같은 코드를 쳤을 때 치는 과정에서 정답이 보이기 시작하는 경지에 다다랐습니다. 처음에는 숙제가 조금 많다는 생각이 들었지만, 끝나고 나니 오히려 많았기 때문에 더욱 숙달될 수 있었다고 생각합니다. 감사합니다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 각 코드를 쳐보며 나왔던 기술적 사항들이나 고찰, 왜 그렇게 나오는가에 대한 생각들을 주석에 달아놓았습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
